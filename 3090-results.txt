### lora
â¯ max=0; while : ; do usage=$(nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits | awk '{print $1}' | sort -nr | head -n 1); if [ "$usage" -gt "$max" ]; then max=$usage; fi; echo $max; sleep 1; done  
4876

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 273/273 [11:41<00:00,  2.54s/it][INFO|trainer.py:1962] 2024-02-18 20:50:55,319 >> 
][INFO|trainer.py:1962] 2024-02-18 20:50:55,319 >> 
/it][INFO|trainer.py:1962] 2024-02-18 20:50:55,319 >> 

  "hidden_act": "silu",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 5632,
  "max_position_embeddings": 2048,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 22,
  "num_key_value_heads": 4,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2433] 2024-02-18 20:50:56,507 >> tokenizer config file saved in test/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-02-18 20:50:56,507 >> Special tokens file saved in test/special_tokens_map.json
***** train metrics *****
  epoch                    =        3.0
  train_loss               =      2.153
  train_runtime            = 0:11:45.57
  train_samples_per_second =      6.186
  train_steps_per_second   =      0.387
Figure saved: test/training_loss.png
02/18/2024 20:50:56 - WARNING - llmtuner.extras.ploting - No metric eval_loss to plot.
[INFO|modelcard.py:452] 2024-02-18 20:50:56,604 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}

### qlora

â¯ max=0; while : ; do usage=$(nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits | awk '{print $1}' | sort -nr | head -n 1); if [ "$usage" -gt "$max" ]; then max=$usage; fi; echo $max; sleep 1; done
3454


[INFO|tokenization_utils_base.py:2433] 2024-02-18 21:10:11,450 >> tokenizer config file saved in test/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-02-18 21:10:11,450 >> Special tokens file saved in test/special_tokens_map.json
***** train metrics *****
  epoch                    =        3.0
  train_loss               =     2.1875
  train_runtime            = 0:11:57.23
  train_samples_per_second =      6.086
  train_steps_per_second   =      0.381
Figure saved: test/training_loss.png
02/18/2024 21:10:11 - WARNING - llmtuner.extras.ploting - No metric eval_loss to plot.
[INFO|modelcard.py:452] 2024-02-18 21:10:11,537 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
wandb: 
wandb: Run history:
wandb:                    train/epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:              train/global_step â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            train/learning_rate â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–†â–†â–†â–…â–…â–„â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–
wandb:                     train/loss â–ˆâ–†â–ƒâ–ˆâ–„â–…â–„â–…â–ƒâ–„â–‡â–ƒâ–†â–‚â–„â–‚â–â–‚â–‚â–„â–ƒâ–‚â–„â–‚â–ƒâ–…â–„
wandb:               train/total_flos â–
wandb:               train/train_loss â–
wandb:            train/train_runtime â–
wandb: train/train_samples_per_second â–
wandb:   train/train_steps_per_second â–
wandb: 
wandb: Run summary:
wandb:                    train/epoch 3.0
wandb:              train/global_step 273
wandb:            train/learning_rate 0.0
wandb:                     train/loss 2.1923
wandb:               train/total_flos 2.777434086703104e+16
wandb:               train/train_loss 2.18747
wandb:            train/train_runtime 717.2368
wandb: train/train_samples_per_second 6.086
wandb:   train/train_steps_per_second 0.381
wandb: 
wandb: ğŸš€ View run brilliant-laughter-34 at: https://wandb.ai/augmxnt/huggingface/runs/52d9avmo
wandb: ï¸âš¡ View job at https://wandb.ai/augmxnt/huggingface/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyNTU5ODkwOQ==/version_details/v14
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20240218_205814-52d9avmo/logs

real    12m40.652s
user    5m51.358s
sys     6m34.945s

### qlora-fa2
â¯ max=0; while : ; do usage=$(nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits | awk '{print $1}' | sort -nr | head -n 1); if [ "$usage" -gt "$max" ]; then max=$usage; fi; echo $max; sleep 1; done
3562

[INFO|tokenization_utils_base.py:2433] 2024-02-18 21:25:23,256 >> tokenizer config file saved in test/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-02-18 21:25:23,256 >> Special tokens file saved in test/special_tokens_map.json
***** train metrics *****
  epoch                    =        3.0
  train_loss               =     2.1873
  train_runtime            = 0:11:28.30
  train_samples_per_second =      6.342
  train_steps_per_second   =      0.397
Figure saved: test/training_loss.png
02/18/2024 21:25:23 - WARNING - llmtuner.extras.ploting - No metric eval_loss to plot.
[INFO|modelcard.py:452] 2024-02-18 21:25:23,337 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
wandb: \ 0.006 MB of 0.006 MB uploaded
wandb: Run history:
wandb:                    train/epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:              train/global_step â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            train/learning_rate â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–†â–†â–†â–…â–…â–„â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–
wandb:                     train/loss â–†â–…â–†â–†â–…â–„â–…â–„â–…â–ˆâ–„â–„â–ƒâ–‚â–…â–…â–„â–â–†â–‚â–ƒâ–„â–â–…â–„â–‚â–„
wandb:               train/total_flos â–
wandb:               train/train_loss â–
wandb:            train/train_runtime â–
wandb: train/train_samples_per_second â–
wandb:   train/train_steps_per_second â–
wandb: 
wandb: Run summary:
wandb:                    train/epoch 3.0
wandb:              train/global_step 273
wandb:            train/learning_rate 0.0
wandb:                     train/loss 2.179
wandb:               train/total_flos 2.777434086703104e+16
wandb:               train/train_loss 2.18732
wandb:            train/train_runtime 688.3085
wandb: train/train_samples_per_second 6.342
wandb:   train/train_steps_per_second 0.397
wandb: 
wandb: ğŸš€ View run vermilion-rocket-35 at: https://wandb.ai/augmxnt/huggingface/runs/jo21062h
wandb: ï¸âš¡ View job at https://wandb.ai/augmxnt/huggingface/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyNTU5ODkwOQ==/version_details/v15
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20240218_211354-jo21062h/logs

real    12m10.618s
user    5m40.811s
sys     6m17.278s


### unsloth

â¯ max=0; while : ; do usage=$(nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits | awk '{print $1}' | sort -nr | head -n 1); if [ "$usage" -gt "$max" ]; then max=$usage; fi; echo $max; sleep 1; done
2540

[INFO|tokenization_utils_base.py:2433] 2024-02-18 22:07:17,838 >> tokenizer config file saved in test/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-02-18 22:07:17,838 >> Special tokens file saved in test/special_tokens_map.json
***** train metrics *****
  epoch                    =        3.0
  train_loss               =     2.1872
  train_runtime            = 0:09:47.32
  train_samples_per_second =      7.432
  train_steps_per_second   =      0.465
Figure saved: test/training_loss.png
02/18/2024 22:07:17 - WARNING - llmtuner.extras.ploting - No metric eval_loss to plot.
[INFO|modelcard.py:452] 2024-02-18 22:07:17,931 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
wandb: 
wandb: Run history:
wandb:                    train/epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:              train/global_step â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:            train/learning_rate â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–†â–†â–†â–…â–…â–„â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–
wandb:                     train/loss â–†â–…â–†â–†â–…â–„â–…â–„â–…â–ˆâ–„â–„â–ƒâ–‚â–…â–…â–„â–â–†â–‚â–ƒâ–„â–â–…â–„â–‚â–„
wandb:               train/total_flos â–
wandb:               train/train_loss â–
wandb:            train/train_runtime â–
wandb: train/train_samples_per_second â–
wandb:   train/train_steps_per_second â–
wandb: 
wandb: Run summary:
wandb:                    train/epoch 3.0
wandb:              train/global_step 273
wandb:            train/learning_rate 0.0
wandb:                     train/loss 2.1787
wandb:               train/total_flos 2.777434086703104e+16
wandb:               train/train_loss 2.18724
wandb:            train/train_runtime 587.3255
wandb: train/train_samples_per_second 7.432
wandb:   train/train_steps_per_second 0.465
wandb: 
wandb: ğŸš€ View run glistening-paper-36 at: https://wandb.ai/augmxnt/huggingface/runs/gs0x94af
wandb: ï¸âš¡ View job at https://wandb.ai/augmxnt/huggingface/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEyNTU5ODkwOQ==/version_details/v16
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)
wandb: Find logs at: ./wandb/run-20240218_215730-gs0x94af/logs

real    10m31.099s
user    5m47.371s
sys     4m29.435s
